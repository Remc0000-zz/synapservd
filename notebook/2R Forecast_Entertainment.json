{
	"name": "2R Forecast_Entertainment",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "SparkPool01",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2"
			}
		},
		"metadata": {
			"saveOutput": true,
			"language_info": {
				"name": "scala"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/506e86fc-853c-4557-a6e5-ad72114efd2b/resourceGroups/CDP-VISION-DEMO-RG/providers/Microsoft.Synapse/workspaces/synretailcdpprod/bigDataPools/SparkPool01",
				"name": "SparkPool01",
				"type": "Spark",
				"endpoint": "https://synretailcdpprod.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/SparkPool01",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "2.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28
			}
		},
		"cells": [
			{
				"cell_type": "markdown",
				"source": [
					"<p style=\"font-size:25px; color:black;\"><u><i><b>Predicting the number of customers likely to visit different departments in a store</b></i></u></p>\n",
					"<p style=\"font-size:16px; color:#117d30;\">\n",
					"    Time series forecasting is the use of a model to predict future values based on previously observed values.\n",
					"The AutoML feature of AzureSynapse, in this case uses more than 25 time series forecasting machine learning algorithms to predicts how many customers are likely to visit different departments in a store.\n",
					"</p>\n",
					"Note:\n",
					"</p>\n",
					"<p style=\"font-size:15px; color:#117d30;\">\n",
					" This notebook is written in Scala, and there is interoperability between Scala and Python code.\n",
					"</p>\n",
					"\n",
					"<p style=\"font-size:15px; color:#117d30;\">\n",
					"    <u> Abstract: </u>\n",
					"</p>\n",
					"<p style=\"font-size:16px; color:#117d30;\">\n",
					"1) Ingest  data from Azure Synapse Data Storage account using PySpark.\n",
					"</p>\n",
					"<p style=\"font-size:16px; color:#117d30;\">\n",
					"2) Exploratory Data Analysis \n",
					"</p>\n",
					"<p style=\"font-size:15px; color:#117d30;\">\n",
					"3) Training more than 25 time series forecasting machine learning algorithms.\n",
					"</p>\n",
					"<p style=\"font-size:15px; color:#117d30;\">\n",
					"4) Predict the number of customers likely to visit different departments in a store by choosing the best performing Machine Learning Algorithm..\n",
					"</p>\n",
					""
				]
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Introduction\n",
					"<p style=\"font-size:16px; color:#117d30;\">\n",
					"\n",
					"\n",
					"### In this notebook we showcased how to:\n",
					"<p style=\"font-size:16px; color:#117d30;\">\n",
					"1. Create an experiment using an existing workspace\n",
					"\n",
					"<p style=\"font-size:16px; color:#117d30;\">\n",
					"2. Configure AutoML using 'AutoMLConfig'\n",
					"\n",
					"<p style=\"font-size:16px; color:#117d30;\">\n",
					"3. Train the model \n",
					"\n",
					"<p style=\"font-size:16px; color:#117d30;\">\n",
					"4. Explore the engineered features and results\n",
					"\n",
					"<p style=\"font-size:16px; color:#117d30;\">\n",
					"5. Configuration and remote run of AutoML for a time-series model with lag and rolling window features\n",
					"\n",
					"<p style=\"font-size:16px; color:#117d30;\">\n",
					"6. Run and explore the forecast\n",
					"\n",
					"<p style=\"font-size:16px; color:#117d30;\">\n",
					"7. Register the model\n",
					"\n",
					"\n",
					"\n",
					""
				]
			},
			{
				"cell_type": "markdown",
				"source": [
					"### Importing required libraries such as azureml, pandas, pandasql, pyspark, and other supporting libraries.\n",
					"\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"%%pyspark\n",
					"import os\n",
					"os.environ['AZURE_SERVICE']=\"Microsoft.ProjectArcadia\""
				],
				"execution_count": 4
			},
			{
				"cell_type": "code",
				"source": [
					"%%pyspark\n",
					"from azureml.train.automl import AutoMLConfig\n",
					"# from azureml.widgets import RunDetails\n",
					"from azureml.core.experiment import Experiment\n",
					"from azureml.core.workspace import Workspace\n",
					"from azureml.train.automl.run import AutoMLRun\n",
					"from sklearn.metrics import mean_squared_error\n",
					"import math\n",
					"from pyspark.sql.window import Window\n",
					"from azureml.core.webservice import AciWebservice\n",
					"from azureml.core.model import InferenceConfig\n",
					"from azureml.core.model import Model\n",
					"from azureml.core.webservice import Webservice\n",
					"from azureml.core.conda_dependencies import CondaDependencies\n",
					"from azureml.core.environment import Environment\n",
					"\n",
					"import pandas as pd \n",
					"import datetime\n",
					"import matplotlib.pyplot as plt\n",
					"import numpy as np \n",
					"import seaborn as sns \n",
					"import azureml.train.automl.runtime\n",
					"import logging\n",
					"import os, tempfile\n",
					"import pandas as pd \n",
					"from pyspark.sql.functions import *\n",
					"from pyspark.sql.types import *\n",
					"from pyspark import SparkContext"
				],
				"execution_count": 5
			},
			{
				"cell_type": "markdown",
				"source": [
					"## *Connecting to Azure Synapse Data Warehouse*\n",
					"<p style=\"font-size:16px; color:#117d30;\">\n",
					"    Connection to Azure Synapse Data Warehouse is initiated and the required data is ingested for processing.\n",
					"    The warehouse is connected with a single line of code. Just point to actions in a table, click on a new notebook, and then click on \"Load to DataFrame\".  </p>\n",
					"   <p style=\"font-size:16px; color:#117d30;\"> After providing the necessary details,  the required data is loaded in the form of a Spark dataframe.\n",
					"One magical line of code converts a dataframe from Scala to Python!\n",
					"</p>\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"val df = spark.read.sqlanalytics(\"SQLPool01.dbo.department_visit_customer\")\n",
					"//Create a Temp view for using the dataframe from Scala to Python\n",
					"  df.createTempView(\"df\")"
				],
				"execution_count": 6
			},
			{
				"cell_type": "code",
				"source": [
					"display(df)"
				],
				"execution_count": 7
			},
			{
				"cell_type": "markdown",
				"source": [
					"# Exploratory Data Analysis\n",
					"\n",
					"<p style=\"font-size:16px; color:#117d30;\">\n",
					"The goal of performing exploratory data analysis is to understand the underlying patterns and correlations among features in the data. \n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"%%pyspark\n",
					"#Calling the dataframe df created in Scala to Python\n",
					"df = sqlContext.table(\"df\")\n",
					"# *********************\n",
					"department_visit_data = df.select(\"*\").toPandas()\n",
					"department_visit_data['Date'] = pd.to_datetime(department_visit_data['Date']).dt.strftime('%m/%d/%y')\n",
					"department_visit_data['Month'] = pd.to_datetime(department_visit_data['Date']).dt.strftime('%m')\n",
					"department_visit_data['DayOfMonth'] = pd.to_datetime(department_visit_data['Date']).dt.strftime('%d')\n",
					"department_visit_data['Year'] = pd.to_datetime(department_visit_data['Date']).dt.strftime('%y')\n",
					"department_visit_data['DayOfWeek'] = pd.to_datetime(department_visit_data['Date']).dt.strftime('%a')\n",
					"department_visit_data[['Accessories_count','Entertainment_count','Gaming','Kids','Mens','Phone_and_GPS','Womens']] = department_visit_data[['Accessories_count','Entertainment_count','Gaming','Kids','Mens','Phone_and_GPS','Womens']].apply(pd.to_numeric)\n",
					"\n",
					"#display(department_visit_data)"
				],
				"execution_count": 8
			},
			{
				"cell_type": "markdown",
				"source": [
					"##  Deriving insights from customer visits data  \n",
					"\n",
					"<p style=\"font-size:16px; color:#117d30;\">\n",
					"1. Heat Map: Thickness of the color indicates the no of customers visiting the section on that particular day. It provides a quick representation of distribution of traffic across days and in various departments. From the graph, we can infer that more number of customers visit the Entertainment department on Wednesdays, Thursdays and Fridays and there is less foot traffic on Mondays and Fridays in the Phone_and_gps department.\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"%%pyspark\n",
					"df_dow = pd.DataFrame(department_visit_data.groupby('DayOfWeek')[['Mens']].sum().sort_values(by = 'DayOfWeek', \n",
					"                                                                 ascending=True))\n",
					"\n",
					"df_dow['Mens'] = pd.DataFrame(department_visit_data.groupby('DayOfWeek')[['Mens']].sum().sort_values(by = 'DayOfWeek', \n",
					"                                                                 ascending=True))\n",
					"\n",
					"df_dow['Womens'] = pd.DataFrame(department_visit_data.groupby('DayOfWeek')[['Womens']].sum().sort_values(by = 'DayOfWeek', \n",
					"                                                                 ascending=True))\n",
					"\n",
					"df_dow['Kids'] = pd.DataFrame(department_visit_data.groupby('DayOfWeek')[['Kids']].sum().sort_values(by = 'DayOfWeek', \n",
					"                                                                 ascending=True))\n",
					"\n",
					"df_dow['Gaming'] = pd.DataFrame(department_visit_data.groupby('DayOfWeek')[['Gaming']].sum().sort_values(by = 'DayOfWeek', \n",
					"                                                                 ascending=True))\n",
					"\n",
					"df_dow['Entertainment'] = pd.DataFrame(department_visit_data.groupby('DayOfWeek')[['Entertainment_count']].sum().sort_values(by = 'DayOfWeek', \n",
					"                                                                 ascending=True))\n",
					"\n",
					"df_dow['Accessories'] = pd.DataFrame(department_visit_data.groupby('DayOfWeek')[['Accessories_count']].sum().sort_values(by = 'DayOfWeek', \n",
					"                                                                 ascending=True))\n",
					"\n",
					"df_dow['Phone_and_GPS'] = pd.DataFrame(department_visit_data.groupby('DayOfWeek')[['Phone_and_GPS']].sum().sort_values(by = 'DayOfWeek', \n",
					"                                                                 ascending=True))\n",
					"\n",
					"df_dow.head(10)\n",
					"\n",
					"sns.set()\n",
					"plt.rcParams['font.size'] = 20\n",
					"bg_color = (0.88,0.85,0.95)\n",
					"plt.rcParams['figure.facecolor'] = bg_color\n",
					"plt.rcParams['axes.facecolor'] = bg_color\n",
					"fig, ax = plt.subplots(1)\n",
					"cmap = sns.diverging_palette(10, 150, n=2, as_cmap=True)\n",
					"\n",
					"p = sns.heatmap(df_dow,\n",
					"                cmap=cmap,\n",
					"                annot=True,\n",
					"                fmt=\"d\",\n",
					"                annot_kws={'size':16},\n",
					"                ax=ax)\n",
					"plt.xlabel('Category')\n",
					"plt.ylabel('Day Of Week')\n",
					"ax.set_ylim((0,7))\n",
					"plt.text(5,7.4, \"Heat Map\", fontsize = 25, color='Black', fontstyle='italic')\n",
					"plt.show()"
				],
				"execution_count": 9
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Cell title\n",
					""
				]
			},
			{
				"cell_type": "markdown",
				"source": [
					"# Data Manipulation  \n",
					"<p style=\"font-size:16px; color:#117d30;\">\n",
					"1. Converting date to a specific format and making date fields relevant for prediction.\n",
					"\n",
					"<p style=\"font-size:16px; color:#117d30;\">\n",
					"2. Converting the data type of the columns to numeric before being passed as input to the model.\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"%%pyspark\n",
					"department_visit_data = df.select(\"*\").toPandas()\n",
					"department_visit_data['Date'] = pd.to_datetime(department_visit_data['Date']).dt.strftime('%Y-%m-%d')\n",
					"department_visit_data[['Accessories_count','Entertainment_count','Gaming','Kids','Mens','Phone_and_GPS','Womens']] = department_visit_data[['Accessories_count','Entertainment_count','Gaming','Kids','Mens','Phone_and_GPS','Womens']].apply(pd.to_numeric)\n",
					"display(department_visit_data)"
				],
				"execution_count": 10
			},
			{
				"cell_type": "code",
				"source": [
					"%%pyspark\n",
					"Entertainment_data = department_visit_data[['Date','Entertainment_count']]\n",
					"display(Entertainment_data)"
				],
				"execution_count": 11
			},
			{
				"cell_type": "code",
				"source": [
					"%%pyspark\n",
					"total_rows = Entertainment_data.count\n",
					"print(total_rows)"
				],
				"execution_count": 12
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Split data into train and test set\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"%%pyspark\n",
					"train_data = pd.DataFrame()\n",
					"test_data = pd.DataFrame()\n",
					"\n",
					"if Entertainment_data.shape[0] > 55: # len(df) > 10 would also work\n",
					"    train_data = Entertainment_data[:55]\n",
					"    test_data = Entertainment_data[55:]"
				],
				"execution_count": 13
			},
			{
				"cell_type": "code",
				"source": [
					"%%pyspark\n",
					"display(train_data)"
				],
				"execution_count": 14
			},
			{
				"cell_type": "code",
				"source": [
					"%%pyspark\n",
					"display(test_data)"
				],
				"execution_count": 55
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Train\n",
					"\n",
					"<p style=\"font-size:16px; color:#117d30;\">\n",
					"1. Instantiate an AutoMLConfig object. \n",
					"<p style=\"font-size:16px; color:#117d30;\">\n",
					"2. The configuration below defines the settings and data used to run the experiment. \n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"%%pyspark\n",
					"subscription_id='49d66a68-7c00-43c3-93ae-602ee60e1eb6'\n",
					"resource_group='CDP-VISION-DEMO-RG'\n",
					"workspace_name='Auto-ML-2'\n",
					"ws = Workspace(subscription_id = subscription_id, resource_group = resource_group, workspace_name = workspace_name)\n",
					"ws.write_config()\n",
					"ws = Workspace.from_config()\n",
					"experiment = Experiment(ws, \"Department_Visit_Count_Prod\")"
				],
				"execution_count": 41
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Set AutoML Configuration Parameters\n",
					"\n",
					"<p style=\"font-size:16px; color:#117d30;\">\n",
					"    The forecast horizon is the number of periods into the future that the model should predict. \n",
					"\n",
					"<p style=\"font-size:16px; color:#117d30;\">\n",
					"    It is generally recommended that users set forecast horizons to less than 100 time periods\n",
					"\n",
					"<p style=\"font-size:16px; color:#117d30;\">\n",
					"    Furthermore, AutoML's memory use and computation time increases in proportion to the length of the horizon, so consider carefully how this value is set. \n",
					"\n",
					"<p style=\"font-size:16px; color:#117d30;\">\n",
					"    If a long horizon forecast really is necessary, consider aggregating the series to a coarser time scale.\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"%%pyspark\n",
					"automl_settings = {\n",
					"   'time_column_name':'Date',\n",
					"   'max_horizon': 25\n",
					"}"
				],
				"execution_count": 56
			},
			{
				"cell_type": "code",
				"source": [
					"%%pyspark\n",
					"automl_config = AutoMLConfig( \n",
					"                            #forecasting for time-series tasks\n",
					"                            task='forecasting',\n",
					"                            #measuere for evaluating the performance of the models\n",
					"                            primary_metric='normalized_root_mean_squared_error',\n",
					"                            #Maximum amount of time in minutes that the experiment take before it terminates.\n",
					"                            experiment_timeout_minutes=15,\n",
					"                            enable_early_stopping=True,\n",
					"                            training_data=train_data,\n",
					"                            label_column_name='Entertainment_count',\n",
					"                            #Rolling Origin Validation is used to split time-series in a temporally consistent way.\n",
					"                            n_cross_validations=4,\n",
					"                            # Flag to enble early termination if the score is not improving in the short term.\n",
					"                            enable_ensembling=False,\n",
					"                            verbosity=logging.INFO,\n",
					"                            **automl_settings)"
				],
				"execution_count": 57
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Run The Experiment\n",
					"<p style=\"font-size:16px; color:#117d30;\">\n",
					"Automated ML runs more than 25 Machine Learning Algorithms and grades them according to performance.\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"%%pyspark\n",
					"automl_experiment_run = experiment.submit(automl_config, show_output=True)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Retrieve the Best Model\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"%%pyspark\n",
					"best_run, fitted_model = automl_experiment_run.get_output()"
				],
				"execution_count": 45
			},
			{
				"cell_type": "code",
				"source": [
					"%%pyspark\n",
					"print(best_run)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"%%pyspark\n",
					"model_name = best_run.properties['model_name']\n",
					"print(model_name)"
				],
				"execution_count": 47
			},
			{
				"cell_type": "code",
				"source": [
					"%%pyspark\n",
					"display(test_data)"
				],
				"execution_count": 54
			},
			{
				"cell_type": "markdown",
				"source": [
					"# Evaluate the Model Performance\n",
					"<p style=\"font-size:16px; color:#117d30;\">Here we have used Root Mean Squared Error (RMSE) for evaluation.</p>\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"%%pyspark\n",
					"test_labels = test_data.pop(\"Entertainment_count\").values\n",
					"predict_labels = fitted_model.predict(test_data)\n",
					"actual_labels = test_labels.flatten()"
				],
				"execution_count": 48
			},
			{
				"cell_type": "code",
				"source": [
					"%%pyspark\n",
					"rmse = math.sqrt(mean_squared_error(actual_labels,predict_labels))\n",
					"rmse"
				],
				"execution_count": 24
			},
			{
				"cell_type": "code",
				"source": [
					"%%pyspark\n",
					"sum_actuals = sum_errors = 0\n",
					"\n",
					"for actual_val, predict_val in zip(actual_labels,predict_labels):\n",
					"    abs_error = actual_val - predict_val\n",
					"    if abs_error < 0:\n",
					"        abs_error = abs_error * -1\n",
					"\n",
					"    sum_errors = sum_errors + abs_error\n",
					"    sum_actuals = sum_actuals + actual_val\n",
					"\n",
					"mean_abs_percent_error = sum_errors / sum_actuals\n",
					"print(\"Model MAPE:\")\n",
					"print(mean_abs_percent_error)\n",
					"print()\n",
					"print(\"Model Accuracy:\")\n",
					"print(1 - mean_abs_percent_error)"
				],
				"execution_count": 49
			},
			{
				"cell_type": "markdown",
				"source": [
					"# Making future prediction using model that performs best\n",
					"\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"%%pyspark\n",
					"predicted_value = fitted_model.predict(test_data)\n",
					"actual_value = test_labels.flatten() \n",
					"actual_value = actual_value.tolist()\n",
					"predicted_value = predicted_value.tolist()\n",
					"output_df= pd.DataFrame({'actual_value':actual_value,'predicted_value':predicted_value})\n",
					"output_df['Error_Rate %'] = 100*((output_df['actual_value']-output_df['predicted_value'])/(output_df['actual_value']))\n",
					"display(output_df)"
				],
				"execution_count": 50
			},
			{
				"cell_type": "code",
				"source": [
					"%%pyspark\n",
					"future_date =  pd.date_range(start='2020-12-1', end='2020-12-5')\n",
					"future_data = pd.DataFrame({'Date':future_date, 'Entertainment_count':0})\n",
					"display(future_data)"
				],
				"execution_count": 27
			},
			{
				"cell_type": "code",
				"source": [
					"%%pyspark\n",
					"future_data['Date'] = pd.to_datetime(future_data['Date']).dt.strftime('%Y-%m-%d')\n",
					"display(future_data)"
				],
				"execution_count": 28
			},
			{
				"cell_type": "code",
				"source": [
					"%%pyspark\n",
					"future_value = fitted_model.predict(future_data)\n",
					"temp_df =  pd.DataFrame({'Entertainment_count':future_value})\n",
					"temp_df['Entertainment_count'] = temp_df['Entertainment_count'].round(0)\n",
					"future_data['Accessories_Customer_count'] = temp_df['Entertainment_count']\n",
					"future_data.drop({'Entertainment_count'},axis=1,inplace=True)\n",
					"future_data['Date'] = pd.to_datetime(future_data['Date']).dt.strftime('%Y-%m-%d')\n",
					"display(future_data)"
				],
				"execution_count": 29
			},
			{
				"cell_type": "code",
				"source": [
					"%%pyspark\n",
					"output = spark.createDataFrame(future_data)"
				],
				"execution_count": 30
			},
			{
				"cell_type": "markdown",
				"source": [
					"## **Registering Model**\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"%%pyspark\n",
					"#register model\n",
					"#model_name = \"my_model_20th\"\n",
					"description = \"Forecast Model\"\n",
					"tags = None\n",
					"model = automl_experiment_run.register_model(description = description, tags = tags)\n",
					"automl_experiment_run.model_id"
				],
				"execution_count": 31
			},
			{
				"cell_type": "code",
				"source": [
					"%%pyspark\n",
					"#saving scoring and conda file\n",
					"script_file_name = 'inference/score.py'\n",
					"conda_env_file_name = 'inference/env.yml'\n",
					"#/content/azureml_automl.log\n",
					"best_run.download_file('outputs/scoring_file_v_1_0_0.py', 'inference/score.py')\n",
					"best_run.download_file('outputs/conda_env_v_1_0_0.yml', 'inference/env.yml')"
				],
				"execution_count": 32
			},
			{
				"cell_type": "code",
				"source": [
					"%%pyspark\n",
					"\n",
					"from azureml.core import Environment\n",
					"from azureml.core.conda_dependencies import CondaDependencies \n",
					"\n",
					"conda = CondaDependencies.create(conda_packages=['numpy>=1.16.0,<=1.16.2','pandas','scikit-learn','py-xgboost<=0.80','fbprophet==0.5','psutil>=5.2.2,<6.0.0'],pip_packages=['azureml-defaults==1.0.83','azureml-train-automl-runtime==1.0.83.1','inference-schema','azureml-explain-model==1.0.83'])\n",
					"\n",
					"myenv=Environment(name=\"automlenv\")\n",
					"myenv.python.conda_dependencies = conda\n",
					""
				],
				"execution_count": 33
			},
			{
				"cell_type": "code",
				"source": [
					"%%pyspark \n",
					"from azureml.core.model import InferenceConfig\n",
					"from azureml.core.webservice import AksWebservice\n",
					"from azureml.core.compute import AksCompute\n",
					"from azureml.core.model import Model\n",
					"\n",
					"aks_name = 'Retaildemo-prod' \n",
					"aks_target = AksCompute(workspace=ws,name=aks_name)\n",
					"\n",
					"aks_config = AksWebservice.deploy_configuration(cpu_cores=1,memory_gb=1,auth_enabled=True)\n",
					"\n",
					"inference_config = InferenceConfig(environment=myenv,\n",
					"                                     entry_script = script_file_name)\n",
					"\n",
					"api_service_name = 'entertainment-forecasting-api'\n",
					"\n",
					"api_service = Model.deploy(workspace=ws,\n",
					"                           name=api_service_name, \n",
					"                           models=[model],\n",
					"                           inference_config=inference_config, \n",
					"                           deployment_config=aks_config,\n",
					"                           deployment_target=aks_target,\n",
					"                           overwrite=True)\n",
					"\n",
					"api_service.wait_for_deployment(show_output=True)"
				],
				"execution_count": 34
			},
			{
				"cell_type": "code",
				"source": [
					"%%pyspark\n",
					"print(api_service.state)"
				],
				"execution_count": 35
			},
			{
				"cell_type": "code",
				"source": [
					"%%pyspark\n",
					"print(api_service.scoring_uri)"
				],
				"execution_count": 36
			},
			{
				"cell_type": "code",
				"source": [
					"%%pyspark\n",
					"import urllib.request\n",
					"import json\n",
					"import os\n",
					"import ssl\n",
					"import pprint\n",
					"\n",
					"\n",
					"def allowSelfSignedHttps(allowed):\n",
					"    # bypass the server certificate verification on client side\n",
					"    if allowed and not os.environ.get('PYTHONHTTPSVERIFY', '') and getattr(ssl, '_create_unverified_context', None):\n",
					"        ssl._create_default_https_context = ssl._create_unverified_context\n",
					"\n",
					"allowSelfSignedHttps(True) # this line is needed if you use self-signed certificate in your scoring service.\n",
					"\n",
					"data = {\n",
					"    \"Inputs\": {\n",
					"          \"WebServiceInput0\":\n",
					"          [\n",
					"              {\n",
					"                    'Date': \"2015-10-01T00:00:00Z\",\n",
					"                    'Accessories_count': \"18\",\n",
					"                    'Entertainment_count': \"28\",\n",
					"                    'Gaming': \"5\",\n",
					"                    'Kids': \"14\",\n",
					"                    'Mens': \"14\",\n",
					"                    'Phone_and_GPS': \"36\",\n",
					"                    'Womens': \"30\",\n",
					"              },\n",
					"          ],\n",
					"    },\n",
					"    \"GlobalParameters\":  {\n",
					"    }\n",
					"}\n",
					"\n",
					"body = str.encode(json.dumps(data))\n",
					"\n",
					"url = 'http://104.209.138.213:80/api/v1/service/retaildemo-prod-inferencing/score'\n",
					"api_key = 'ka5cntwm4geSdtvmiqLL7964U9REDpSQ' # Replace this with the API key for the web service\n",
					"headers = {'Content-Type':'application/json', 'Authorization':('Bearer '+ api_key)}\n",
					"\n",
					"req = urllib.request.Request(url, body, headers)\n",
					"\n",
					"try:\n",
					"    response = urllib.request.urlopen(req)\n",
					"\n",
					"    result = response.read()\n",
					"    #print(result)\n",
					"    pprint.pprint(json.loads(result))\n",
					"except urllib.error.HTTPError as error:\n",
					"    print(\"The request failed with status code: \" + str(error.code))\n",
					"\n",
					"    # Print the headers - they include the requert ID and the timestamp, which are useful for debugging the failure\n",
					"    print(error.info())\n",
					"    print(json.loads(error.read().decode(\"utf8\", 'ignore')))\n",
					""
				],
				"execution_count": 37
			},
			{
				"cell_type": "code",
				"source": [
					"%%pyspark\n",
					"import urllib.request\n",
					"import json\n",
					"import os\n",
					"import ssl\n",
					"import pprint\n",
					"\n",
					"\n",
					"def allowSelfSignedHttps(allowed):\n",
					"    # bypass the server certificate verification on client side\n",
					"    if allowed and not os.environ.get('PYTHONHTTPSVERIFY', '') and getattr(ssl, '_create_unverified_context', None):\n",
					"        ssl._create_default_https_context = ssl._create_unverified_context\n",
					"\n",
					"allowSelfSignedHttps(True) # this line is needed if you use self-signed certificate in your scoring service.\n",
					"\n",
					"data = {\n",
					"    \"Inputs\": {\n",
					"          \"WebServiceInput0\":\n",
					"          [\n",
					"              {\n",
					"                    'Date': \"2015-10-01T00:00:00Z\",\n",
					"                    'Accessories_count': \"18\",\n",
					"                    'Entertainment_count': \"28\",\n",
					"                    'Gaming': \"5\",\n",
					"                    'Kids': \"14\",\n",
					"                    'Mens': \"14\",\n",
					"                    'Phone_and_GPS': \"36\",\n",
					"                    'Womens': \"30\",\n",
					"              },\n",
					"          ],\n",
					"    },\n",
					"    \"GlobalParameters\":  {\n",
					"    }\n",
					"}\n",
					"\n",
					"body = str.encode(json.dumps(data))\n",
					"\n",
					"url = 'http://104.209.138.213:80/api/v1/service/entertainment-forecasting-api/score'\n",
					"api_key = 'JFHw1RzAHcHtUCav9FM3jg9e5q3cGZ64' # Replace this with the API key for the web service\n",
					"headers = {'Content-Type':'application/json', 'Authorization':('Bearer '+ api_key)}\n",
					"\n",
					"req = urllib.request.Request(url, body, headers)\n",
					"\n",
					"try:\n",
					"    response = urllib.request.urlopen(req)\n",
					"\n",
					"    result = response.read()\n",
					"    #print(result)\n",
					"    pprint.pprint(json.loads(result))\n",
					"except urllib.error.HTTPError as error:\n",
					"    print(\"The request failed with status code: \" + str(error.code))\n",
					"\n",
					"    # Print the headers - they include the requert ID and the timestamp, which are useful for debugging the failure\n",
					"    print(error.info())\n",
					"    print(json.loads(error.read().decode(\"utf8\", 'ignore')))\n",
					""
				],
				"execution_count": 38
			},
			{
				"cell_type": "code",
				"source": [
					"%%pyspark\n",
					"import json\n",
					"\n",
					"api_service_name = 'entertainment-forecasting-api'\n",
					"forecasting_api = Webservice(ws, api_service_name)\n",
					"\n",
					"#predict all days of a future 2 weeks: max horizon was set to 24. \n",
					"#create a chart\n",
					"#point out that the predictiosn match the historical view\n",
					"#do one that shows Thursday high traffic\n",
					"#show Monday shows slow traffic\n",
					"\n",
					"entertainment_forecast_data = json.dumps({'data': [\n",
					"    [2018-20-1,1]\n",
					"]})\n",
					"\n",
					"forecasting_api.run(input_data=entertainment_forecast_data)"
				],
				"execution_count": 39
			},
			{
				"cell_type": "code",
				"source": [
					""
				],
				"execution_count": 40
			}
		]
	}
}